{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c8a437",
   "metadata": {},
   "source": [
    "# Policy iteration #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f56cf",
   "metadata": {},
   "source": [
    " - Here, instead of converging the value table, we will focus on the policy, pi table\n",
    " - pi table - an array that stores the best action at each state\n",
    " - this should be more efficient than the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edf922",
   "metadata": {},
   "source": [
    " - first copying in the same action to define our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "choice = np.array([[0,-1], [0,+1], [-1,0],[+1,0]])\n",
    "\n",
    "action = np.zeros((25,4))\n",
    "\n",
    "for x in range (0,5):\n",
    "    for y in range(0,5):\n",
    "\n",
    "        s = x + 5*y\n",
    "        for a in range (0,4):\n",
    "        \n",
    "        #0=up(0,-1)\n",
    "        #1=down(0,+1)\n",
    "        #2=left(-1,0)\n",
    "        #3=right(+1,0)\n",
    "\n",
    "            next_x = x + choice[a,0]\n",
    "            next_y = y + choice[a,1]\n",
    "            action[s,a] = next_x + 5*next_y\n",
    "\n",
    "            if next_x < 0 or next_x >4 or next_y < 0 or next_y >4:\n",
    "\n",
    "                # slightly different to Q-table, I will not permit an action that goes out of bounds\n",
    "                action[s,a] = 0\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e6c77",
   "metadata": {},
   "source": [
    " - intialize an empty value_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26098c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = np.zeros(25)\n",
    "# pi table will literally be the index of the best action for each state\n",
    "pi_table = np.zeros(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64094962",
   "metadata": {},
   "source": [
    " - make a function that finds the best action at each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(state, action, value_table): \n",
    "\n",
    "    # need to filter out the actions that lead to 0, which is the out of bounds action\n",
    "    possible_actions = []\n",
    "    for i in range(0,4):\n",
    "        next_state = int(action[state,i])\n",
    "        if action[state,i] != 0 and state != 1 and state != 5:\n",
    "            possible_actions.append(i)\n",
    "\n",
    "    next_state_value = [value_table[int(action[state,possible_actions[j]])] for j in range(0,len(possible_actions))]\n",
    "    best_action_index = np.argmax(next_state_value)\n",
    "    best_action = possible_actions[best_action_index]\n",
    "\n",
    "    return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_update(value_table, action, discount_rate=0.9):\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
